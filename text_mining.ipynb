{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import morfeusz2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "import itertools as it\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korpus dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = (\"./streszczenia_original\")\n",
    "corpus = PlaintextCorpusReader(corpus_dir, \".*\\.txt\")\n",
    "files_names = corpus.fileids()\n",
    "files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstępne przygotowanie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for file in files_names:\n",
    "    documents[file] = corpus.raw(file)\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist_file = open(\"./stopwords_pl.txt\", \"r\", encoding=\"UTF-8\")\n",
    "stoplist = stoplist_file.read().splitlines()\n",
    "stoplist_file.close()\n",
    "stoplist = stoplist[4:]\n",
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "    segments = it.groupby(morf.analyse(text), op.itemgetter(0,1))\n",
    "    def disambiguate(group):\n",
    "        pairs = ((len(descr), lemma) for _, _, (_, lemma, descr, _, _, ) in group)\n",
    "        perpl, lemma = min(pairs)\n",
    "        return lemma.split(\":\")[0]\n",
    "    lemmas = (disambiguate(group) for key, group in segments)\n",
    "    return \" \".join(filter(str.isalpha, lemmas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in documents:\n",
    "    documents[key] = documents[key].lower()\n",
    "    documents[key] = \"\".join([char for char in documents[key] if char not in string.punctuation])\n",
    "    documents[key] = lemmatize(documents[key])\n",
    "    documents[key] = \" \".join([word for word in word_tokenize(documents[key], language=\"polish\") if word not in stoplist])\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utworzenie macierzy częstości "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame.from_dict(documents, orient='index')\n",
    "docs.columns = ['content']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "counts_tf = count_vectorizer.fit_transform(docs['content'])\n",
    "counts_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "counts_tfidf = tfidf_vectorizer.fit_transform(docs['content'])\n",
    "counts_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chmury tagów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=5000,\n",
    "    contour_width=3, \n",
    "    contour_color=\"steelblue\"\n",
    ")\n",
    "for index, row in docs.iterrows():\n",
    "    wordcloud.generate(row['content'])\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(index.replace(\".txt\", \"\"))\n",
    "    plt.savefig(\"./wordclouds/{}\".format(index.replace(\".txt\", \".png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title, subplots):\n",
    "    fig, axes = plt.subplots(*subplots, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.savefig(f\"./topics/{title}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "n_top_features = 20\n",
    "size = [1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(counts_tf)\n",
    "plot_top_words(lda, feature_names, n_top_features, \"Tematy w modelu LDA\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_fm = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1, \n",
    "    alpha_H=.00005,\n",
    "    alpha_W=.00005,\n",
    "    l1_ratio=.5\n",
    ")\n",
    "nmf_fm.fit(counts_tfidf)\n",
    "plot_top_words(nmf_fm, feature_names, n_top_features, \"Tematy w modelu NMF Norma Macierzowa\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_kl = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1, \n",
    "    beta_loss='kullback-leibler',\n",
    "    solver='mu',\n",
    "    max_iter=1000,\n",
    "    alpha_H=.00005,\n",
    "    alpha_W=.00005,\n",
    "    l1_ratio=.5\n",
    ")\n",
    "nmf_kl.fit(counts_tfidf)\n",
    "plot_top_words(nmf_kl, feature_names, n_top_features, \"Tematy w modelu NMF Uogulniona Dywergencja\", size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
